{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26028, 39)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql_table('DisasterResponse',con = engine)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Weather update - a cold front from Cuba that could pass over Haiti'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['message'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Un front froid se retrouve sur Cuba ce matin. Il pourrait traverser Haiti demain. Des averses de pluie isolee sont encore prevues sur notre region ce soi'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['original'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>military</th>\n",
       "      <th>water</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   related  request  offer  aid_related  medical_help  medical_products  \\\n",
       "0        1        0      0            0             0                 0   \n",
       "1        1        0      0            1             0                 0   \n",
       "2        1        0      0            0             0                 0   \n",
       "\n",
       "   search_and_rescue  security  military  water      ...        aid_centers  \\\n",
       "0                  0         0         0      0      ...                  0   \n",
       "1                  0         0         0      0      ...                  0   \n",
       "2                  0         0         0      0      ...                  0   \n",
       "\n",
       "   other_infrastructure  weather_related  floods  storm  fire  earthquake  \\\n",
       "0                     0                0       0      0     0           0   \n",
       "1                     0                1       0      1     0           0   \n",
       "2                     0                0       0      0     0           0   \n",
       "\n",
       "   cold  other_weather  direct_report  \n",
       "0     0              0              0  \n",
       "1     0              0              0  \n",
       "2     0              0              0  \n",
       "\n",
       "[3 rows x 35 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['message']\n",
    "Y = df.drop(['id', 'message', 'original', 'genre'], axis = 1)\n",
    "Y.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    ''' Natural Language Processing: Normalize, Tokenize, Stem/Lemmatize\n",
    "    '''\n",
    "    \n",
    "    # Convert text to lowercase and remove punctuation\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    # Tokenize words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove Stop words, Stem & Lemmed words\n",
    "    stop_word = stopwords.words(\"english\")\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    #stemmed = [stemmer.stem(w) for w in tokens if w not in stop_word]\n",
    "    lemmed = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_word]\n",
    "    \n",
    "    return lemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "- You'll find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables.\n",
    "\n",
    "Multi target classification:\n",
    "This strategy consists of fitting one classifier per target. This is a simple strategy for extending classifiers that do not natively support multi-target classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state = 0)\n",
    "\n",
    "np.random.seed(0)\n",
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred = pipeline.predict(X_train)\n",
    "Y_test_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.99      0.99      0.99     13939\n",
      "               request       1.00      0.92      0.96      3094\n",
      "                 offer       1.00      0.69      0.81        83\n",
      "           aid_related       1.00      0.97      0.98      7539\n",
      "          medical_help       1.00      0.85      0.92      1416\n",
      "      medical_products       1.00      0.84      0.91       881\n",
      "     search_and_rescue       1.00      0.83      0.91       491\n",
      "              security       0.99      0.77      0.87       320\n",
      "              military       1.00      0.82      0.90       597\n",
      "                 water       1.00      0.92      0.96      1135\n",
      "                  food       1.00      0.96      0.98      2038\n",
      "               shelter       1.00      0.92      0.96      1611\n",
      "              clothing       1.00      0.82      0.90       278\n",
      "                 money       1.00      0.81      0.89       435\n",
      "        missing_people       0.99      0.73      0.84       201\n",
      "              refugees       1.00      0.80      0.89       610\n",
      "                 death       1.00      0.89      0.94       837\n",
      "             other_aid       1.00      0.85      0.92      2403\n",
      "infrastructure_related       1.00      0.78      0.88      1201\n",
      "             transport       1.00      0.81      0.89       843\n",
      "             buildings       1.00      0.87      0.93       935\n",
      "           electricity       1.00      0.79      0.89       389\n",
      "                 tools       1.00      0.83      0.90       115\n",
      "             hospitals       1.00      0.70      0.83       202\n",
      "                 shops       1.00      0.72      0.84        90\n",
      "           aid_centers       1.00      0.77      0.87       212\n",
      "  other_infrastructure       1.00      0.76      0.86       814\n",
      "       weather_related       0.99      0.95      0.97      5075\n",
      "                floods       1.00      0.92      0.96      1517\n",
      "                 storm       1.00      0.94      0.97      1700\n",
      "                  fire       1.00      0.78      0.88       200\n",
      "            earthquake       1.00      0.96      0.98      1695\n",
      "                  cold       1.00      0.81      0.90       368\n",
      "         other_weather       1.00      0.80      0.89       962\n",
      "         direct_report       1.00      0.91      0.95      3532\n",
      "\n",
      "           avg / total       1.00      0.92      0.96     57758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "col_names = Y.columns.tolist()\n",
    "print(classification_report(Y_train, Y_train_pred, target_names= col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.84      0.92      0.88      5967\n",
      "               request       0.83      0.45      0.58      1380\n",
      "                 offer       0.00      0.00      0.00        35\n",
      "           aid_related       0.76      0.60      0.67      3321\n",
      "          medical_help       0.69      0.10      0.17       668\n",
      "      medical_products       0.76      0.09      0.16       432\n",
      "     search_and_rescue       0.68      0.06      0.12       233\n",
      "              security       0.33      0.01      0.01       151\n",
      "              military       0.75      0.05      0.09       263\n",
      "                 water       0.87      0.26      0.40       537\n",
      "                  food       0.80      0.56      0.66       885\n",
      "               shelter       0.78      0.35      0.48       703\n",
      "              clothing       0.79      0.12      0.21       127\n",
      "                 money       0.38      0.03      0.05       169\n",
      "        missing_people       0.00      0.00      0.00        97\n",
      "              refugees       0.30      0.01      0.02       265\n",
      "                 death       0.81      0.17      0.28       357\n",
      "             other_aid       0.49      0.06      0.10      1043\n",
      "infrastructure_related       0.33      0.01      0.02       504\n",
      "             transport       0.63      0.09      0.15       358\n",
      "             buildings       0.83      0.11      0.20       398\n",
      "           electricity       0.67      0.04      0.08       143\n",
      "                 tools       0.00      0.00      0.00        44\n",
      "             hospitals       1.00      0.01      0.02        81\n",
      "                 shops       0.00      0.00      0.00        30\n",
      "           aid_centers       0.00      0.00      0.00        97\n",
      "  other_infrastructure       0.33      0.01      0.01       337\n",
      "       weather_related       0.84      0.62      0.71      2222\n",
      "                floods       0.89      0.40      0.55       638\n",
      "                 storm       0.72      0.39      0.51       743\n",
      "                  fire       0.67      0.02      0.05        82\n",
      "            earthquake       0.90      0.70      0.78       760\n",
      "                  cold       0.88      0.09      0.17       162\n",
      "         other_weather       0.42      0.03      0.05       414\n",
      "         direct_report       0.74      0.33      0.46      1543\n",
      "\n",
      "           avg / total       0.75      0.49      0.54     25189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, Y_test_pred, target_names= col_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metric(y_true, y_pred):\n",
    "    \"\"\"Calculate median F1 score for all of the output classifiers\n",
    "    Returns:\n",
    "    score: float. Median F1 score for all of the output classifiers\n",
    "    \"\"\"\n",
    "    f1_list = []\n",
    "    for i in range(np.shape(y_pred)[1]):\n",
    "        f1 = f1_score(np.array(y_true)[:, i], y_pred[:, i])\n",
    "        f1_list.append(f1)\n",
    "        \n",
    "    score = np.median(f1_list)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.1385281385281385, total=  43.7s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   55.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.16143497757847533, total=  43.7s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.14468085106382977, total=  43.0s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  2.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=10, score=0.17699115044247787, total=  39.6s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  3.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=10, score=0.2109704641350211, total=  39.9s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  4.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=10, score=0.2119205298013245, total=  40.1s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  5.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.1639344262295082, total=  42.6s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  6.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.21631205673758863, total=  43.3s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  7.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.21408450704225354, total=  43.1s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  8.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=10, score=0.1845238095238095, total=  39.1s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=10, score=0.21750663129973472, total=  39.8s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=10, score=0.22628951747088186, total=  39.0s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.2005899705014749, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.24291497975708504, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.17872340425531916, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=10, score=0.1623931623931624, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=10, score=0.22162162162162163, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=10, score=0.2162162162162162, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.26628895184135976, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.20708446866485014, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.20477815699658702, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=10, score=0.19999999999999998, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=10, score=0.23688663282571912, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=10, score=0.20512820512820512, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.2231075697211155, total=  37.7s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.17761989342806397, total=  38.0s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.1931818181818182, total=  37.2s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=10, score=0.24409448818897636, total=  35.6s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=10, score=0.21114369501466276, total=  35.4s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=10, score=0.21630615640599, total=  35.3s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.19277108433734938, total=  37.6s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.21399176954732513, total=  37.6s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.17449664429530198, total=  37.2s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=10, score=0.22310756972111556, total=  34.8s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=10, score=0.2017291066282421, total=  34.9s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=10, score=0.15172413793103448, total=  34.8s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.22131147540983606, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.22123893805309733, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.20945945945945946, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=10, score=0.22439024390243906, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=10, score=0.17582417582417584, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=10, score=0.21254355400696867, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.23770491803278687, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.1896551724137931, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.16806722689075632, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=10, score=0.21487603305785125, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=10, score=0.22695035460992907, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=10, score=0.21612349914236706, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.18978102189781024, total=  36.4s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.21978021978021978, total=  36.4s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.24836601307189543, total=  35.6s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=10, score=0.2222222222222222, total=  34.0s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=10, score=0.24615384615384617, total=  34.0s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=10, score=0.22099447513812157, total=  34.5s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.23771790808240892, total=  35.1s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.20408163265306123, total=  35.4s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.18618618618618618, total=  35.4s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=10, score=0.22418879056047197, total=  33.0s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=10, score=0.22739018087855295, total=  33.2s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=10, score=0.19008264462809918, total=  32.9s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.19834710743801653, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.2352941176470588, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.19596541786743518, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=10, score=0.24539877300613497, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=10, score=0.19555555555555554, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=10, score=0.1909722222222222, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.19858156028368792, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.21794871794871792, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.1639344262295082, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=10, score=0.22073578595317725, total= 1.0min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=10, score=0.20494699646643108, total= 1.0min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=10 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=10, score=0.18815331010452963, total= 1.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed: 81.5min finished\n"
     ]
    }
   ],
   "source": [
    "parameters = {'vect__min_df': [5,10],\n",
    "              'tfidf__use_idf':[True, False],\n",
    "              'clf__estimator__n_estimators':[10, 25], \n",
    "              'clf__estimator__min_samples_split':[2, 5, 10]}\n",
    "\n",
    "scorer = make_scorer(performance_metric)\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters, scoring = scorer, verbose = 10)\n",
    "\n",
    "# Find best parameters\n",
    "np.random.seed(0)\n",
    "tuned_model = cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 37.61458715,  34.29288705,  37.37542637,  33.69403235,\n",
       "         79.54936131,  71.76845209,  79.3786033 ,  69.99145404,\n",
       "         32.03317817,  29.93887281,  31.76294859,  29.365695  ,\n",
       "         66.23909179,  62.38600246,  65.33809566,  60.0961473 ,\n",
       "         30.49291142,  28.63490717,  29.59808421,  27.53292481,\n",
       "         61.74419681,  57.21141521,  59.77261329,  54.99422868]),\n",
       " 'std_fit_time': array([ 0.29438319,  0.24339276,  0.33642321,  0.33448901,  1.01911124,\n",
       "         0.45252358,  0.24915408,  0.4156832 ,  0.31869422,  0.05530541,\n",
       "         0.07114657,  0.09492371,  0.44695   ,  0.82088084,  0.55795519,\n",
       "         0.27514058,  0.34530312,  0.25268047,  0.21482319,  0.11769937,\n",
       "         0.17700611,  0.22159254,  0.18039763,  0.17811765]),\n",
       " 'mean_score_time': array([ 5.85052617,  5.58566141,  5.62301071,  5.60202726,  7.45897738,\n",
       "         7.21622229,  7.34170143,  7.04663809,  5.62263862,  5.50233587,\n",
       "         5.70539935,  5.47167516,  7.33288272,  7.39038674,  7.3667078 ,\n",
       "         6.95737751,  5.65612173,  5.53970949,  5.69572878,  5.48947938,\n",
       "         7.42156839,  7.04725933,  7.3375593 ,  6.99617847]),\n",
       " 'std_score_time': array([ 0.12905317,  0.04008899,  0.05478907,  0.04774621,  0.01461336,\n",
       "         0.16655574,  0.06217967,  0.01837486,  0.05442379,  0.07185125,\n",
       "         0.09420766,  0.0500572 ,  0.09349578,  0.43094999,  0.12581632,\n",
       "         0.0385739 ,  0.07945731,  0.0488534 ,  0.08429316,  0.05223423,\n",
       "         0.19229952,  0.04871744,  0.12029437,  0.08164494]),\n",
       " 'param_clf__estimator__min_samples_split': masked_array(data = [2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 10 10 10 10 10 10 10 10],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_clf__estimator__n_estimators': masked_array(data = [10 10 10 10 25 25 25 25 10 10 10 10 25 25 25 25 10 10 10 10 25 25 25 25],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_tfidf__use_idf': masked_array(data = [True True False False True True False False True True False False True\n",
       "  True False False True True False False True True False False],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__min_df': masked_array(data = [5 10 5 10 5 10 5 10 5 10 5 10 5 10 5 10 5 10 5 10 5 10 5 10],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 10},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 10},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 10},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 10},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 10},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 10},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 10},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 10},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 10},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 10},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 10},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 10}],\n",
       " 'split0_test_score': array([ 0.13852814,  0.17699115,  0.16393443,  0.18452381,  0.20058997,\n",
       "         0.16239316,  0.26628895,  0.2       ,  0.22310757,  0.24409449,\n",
       "         0.19277108,  0.22310757,  0.22131148,  0.22439024,  0.23770492,\n",
       "         0.21487603,  0.18978102,  0.22222222,  0.23771791,  0.22418879,\n",
       "         0.19834711,  0.24539877,  0.19858156,  0.22073579]),\n",
       " 'split1_test_score': array([ 0.16143498,  0.21097046,  0.21631206,  0.21750663,  0.24291498,\n",
       "         0.22162162,  0.20708447,  0.23688663,  0.17761989,  0.2111437 ,\n",
       "         0.21399177,  0.20172911,  0.22123894,  0.17582418,  0.18965517,\n",
       "         0.22695035,  0.21978022,  0.24615385,  0.20408163,  0.22739018,\n",
       "         0.23529412,  0.19555556,  0.21794872,  0.204947  ]),\n",
       " 'split2_test_score': array([ 0.14468085,  0.21192053,  0.21408451,  0.22628952,  0.1787234 ,\n",
       "         0.21621622,  0.20477816,  0.20512821,  0.19318182,  0.21630616,\n",
       "         0.17449664,  0.15172414,  0.20945946,  0.21254355,  0.16806723,\n",
       "         0.2161235 ,  0.24836601,  0.22099448,  0.18618619,  0.19008264,\n",
       "         0.19596542,  0.19097222,  0.16393443,  0.18815331]),\n",
       " 'mean_test_score': array([ 0.14821466,  0.19996071,  0.19811033,  0.20943999,  0.20740945,\n",
       "         0.200077  ,  0.22605053,  0.21400495,  0.19796976,  0.22384811,\n",
       "         0.19375317,  0.19218694,  0.21733662,  0.20425266,  0.19847577,\n",
       "         0.21931663,  0.21930908,  0.22979018,  0.20932858,  0.21388721,\n",
       "         0.20986888,  0.21064218,  0.19348823,  0.20461203]),\n",
       " 'std_test_score': array([ 0.00967976,  0.01624657,  0.02418312,  0.01797956,  0.02664606,\n",
       "         0.02673772,  0.02846844,  0.01631468,  0.01887636,  0.01447065,\n",
       "         0.01613876,  0.02991308,  0.00557008,  0.02067559,  0.0291056 ,\n",
       "         0.00542183,  0.02391954,  0.01158171,  0.02136239,  0.01688303,\n",
       "         0.01800463,  0.02464775,  0.02234342,  0.01330385]),\n",
       " 'rank_test_score': array([24, 17, 19, 11, 13, 16,  2,  7, 20,  3, 21, 23,  6, 15, 18,  4,  5,\n",
       "         1, 12,  8, 10,  9, 22, 14], dtype=int32),\n",
       " 'split0_train_score': array([ 0.91758242,  0.91730605,  0.91275168,  0.91079812,  0.98174953,\n",
       "         0.97959184,  0.98333333,  0.98045397,  0.82315113,  0.81818182,\n",
       "         0.8201005 ,  0.8343949 ,  0.86465433,  0.85176471,  0.85      ,\n",
       "         0.85265226,  0.74489796,  0.7260274 ,  0.72222222,  0.75242718,\n",
       "         0.77078086,  0.77441077,  0.74358974,  0.74034335]),\n",
       " 'split1_train_score': array([ 0.91891892,  0.91845494,  0.91927083,  0.91447736,  0.98466454,\n",
       "         0.981755  ,  0.98241126,  0.98051606,  0.84463895,  0.84645669,\n",
       "         0.8245614 ,  0.80651731,  0.86425339,  0.86129971,  0.85714286,\n",
       "         0.84581498,  0.76626506,  0.73963868,  0.74788136,  0.74698795,\n",
       "         0.77835588,  0.77580813,  0.77018634,  0.75847458]),\n",
       " 'split2_train_score': array([ 0.92369021,  0.90909091,  0.91358025,  0.91196388,  0.98266625,\n",
       "         0.98461538,  0.98372514,  0.98327548,  0.82      ,  0.84210526,\n",
       "         0.82445759,  0.82254697,  0.87818697,  0.86989554,  0.849642  ,\n",
       "         0.86588235,  0.77931034,  0.74869656,  0.75183246,  0.73958333,\n",
       "         0.7839196 ,  0.77319588,  0.75618375,  0.74658254]),\n",
       " 'mean_train_score': array([ 0.92006385,  0.91495063,  0.91520092,  0.91241312,  0.98302677,\n",
       "         0.98198741,  0.98315658,  0.98141517,  0.82926336,  0.83558126,\n",
       "         0.82303983,  0.82115306,  0.86903157,  0.86098665,  0.85226162,\n",
       "         0.8547832 ,  0.76349112,  0.73812088,  0.74064535,  0.74633282,\n",
       "         0.77768544,  0.77447159,  0.75665327,  0.74846682]),\n",
       " 'std_train_score': array([ 0.00262163,  0.00416991,  0.00289768,  0.00153527,  0.00121705,\n",
       "         0.00205743,  0.00055076,  0.00131568,  0.01094803,  0.01243085,\n",
       "         0.00207885,  0.01142358,  0.00647592,  0.00740519,  0.00345465,\n",
       "         0.00832989,  0.01418507,  0.00931667,  0.0131266 ,  0.0052639 ,\n",
       "         0.00538478,  0.00106732,  0.01086309,  0.007521  ])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22979018117139663"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best mean test score\n",
    "np.max(tuned_model.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__min_samples_split': 10,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__min_df': 10}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for best mean test score\n",
    "tuned_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.85      0.93      0.89      5967\n",
      "               request       0.79      0.50      0.61      1380\n",
      "                 offer       0.00      0.00      0.00        35\n",
      "           aid_related       0.73      0.72      0.72      3321\n",
      "          medical_help       0.59      0.18      0.27       668\n",
      "      medical_products       0.76      0.17      0.28       432\n",
      "     search_and_rescue       0.67      0.15      0.25       233\n",
      "              security       0.17      0.01      0.01       151\n",
      "              military       0.60      0.14      0.23       263\n",
      "                 water       0.86      0.45      0.59       537\n",
      "                  food       0.80      0.66      0.72       885\n",
      "               shelter       0.78      0.41      0.54       703\n",
      "              clothing       0.65      0.16      0.25       127\n",
      "                 money       0.67      0.08      0.15       169\n",
      "        missing_people       1.00      0.01      0.02        97\n",
      "              refugees       0.63      0.09      0.16       265\n",
      "                 death       0.76      0.31      0.44       357\n",
      "             other_aid       0.50      0.08      0.14      1043\n",
      "infrastructure_related       0.04      0.00      0.00       504\n",
      "             transport       0.60      0.14      0.23       358\n",
      "             buildings       0.79      0.21      0.34       398\n",
      "           electricity       0.76      0.11      0.20       143\n",
      "                 tools       0.00      0.00      0.00        44\n",
      "             hospitals       0.00      0.00      0.00        81\n",
      "                 shops       0.00      0.00      0.00        30\n",
      "           aid_centers       0.00      0.00      0.00        97\n",
      "  other_infrastructure       0.50      0.01      0.02       337\n",
      "       weather_related       0.82      0.74      0.78      2222\n",
      "                floods       0.88      0.44      0.58       638\n",
      "                 storm       0.72      0.61      0.66       743\n",
      "                  fire       0.83      0.06      0.11        82\n",
      "            earthquake       0.90      0.83      0.86       760\n",
      "                  cold       0.67      0.20      0.30       162\n",
      "         other_weather       0.43      0.06      0.10       414\n",
      "         direct_report       0.69      0.38      0.49      1543\n",
      "\n",
      "           avg / total       0.73      0.56      0.60     25189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuned_test_pred = tuned_model.predict(X_test)\n",
    "print(classification_report(Y_test, tuned_test_pred, target_names= col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.92      0.88      5967\n",
      "          1       0.83      0.45      0.58      1380\n",
      "          2       0.00      0.00      0.00        35\n",
      "          3       0.76      0.60      0.67      3321\n",
      "          4       0.69      0.10      0.17       668\n",
      "          5       0.76      0.09      0.16       432\n",
      "          6       0.68      0.06      0.12       233\n",
      "          7       0.33      0.01      0.01       151\n",
      "          8       0.75      0.05      0.09       263\n",
      "          9       0.87      0.26      0.40       537\n",
      "         10       0.80      0.56      0.66       885\n",
      "         11       0.78      0.35      0.48       703\n",
      "         12       0.79      0.12      0.21       127\n",
      "         13       0.38      0.03      0.05       169\n",
      "         14       0.00      0.00      0.00        97\n",
      "         15       0.30      0.01      0.02       265\n",
      "         16       0.81      0.17      0.28       357\n",
      "         17       0.49      0.06      0.10      1043\n",
      "         18       0.33      0.01      0.02       504\n",
      "         19       0.63      0.09      0.15       358\n",
      "         20       0.83      0.11      0.20       398\n",
      "         21       0.67      0.04      0.08       143\n",
      "         22       0.00      0.00      0.00        44\n",
      "         23       1.00      0.01      0.02        81\n",
      "         24       0.00      0.00      0.00        30\n",
      "         25       0.00      0.00      0.00        97\n",
      "         26       0.33      0.01      0.01       337\n",
      "         27       0.84      0.62      0.71      2222\n",
      "         28       0.89      0.40      0.55       638\n",
      "         29       0.72      0.39      0.51       743\n",
      "         30       0.67      0.02      0.05        82\n",
      "         31       0.90      0.70      0.78       760\n",
      "         32       0.88      0.09      0.17       162\n",
      "         33       0.42      0.03      0.05       414\n",
      "         34       0.74      0.33      0.46      1543\n",
      "\n",
      "avg / total       0.75      0.49      0.54     25189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, Y_test_pred, target_names= col_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(database_filepath):\n",
    "    engine = create_engine('sqlite:///' + database_filepath)\n",
    "    df = pd.read_sql_table ('DisasterResponse', con = engine)\n",
    "    X = df['message']\n",
    "    Y = df.drop(['id', 'message', 'original', 'genre'], axis = 1)\n",
    "    col_names = Y.columns.tolist()\n",
    "return X,Y,category_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(estimator = AdaBoostClassifier()))\n",
    "    ])\n",
    "    \n",
    "    parameters = {\n",
    "        'vect__min_df':[1,10,50],\n",
    "        'clf__estimator__learning_rate': [0.001, 0.01, 0.1],\n",
    "        'tfidf__smooth_idf': [True, False]\n",
    "    }\n",
    "    \n",
    "    model  = GridSearchCV(pipeline, param_grid = parameters, cv=2) \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, Y_test, category_names):\n",
    "    Y_pred = model.predict(X_test)\n",
    "    print(classification_report(Y_test, Y_pred, target_names=category_names, digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_filepath):\n",
    "    with open(model_filepath, 'wb') as pkl_file:\n",
    "        pickle.dump(model, pkl_file)\n",
    "    pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    X, Y, category_names = load_data(database_filepath)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3) \n",
    "    model = build_model()\n",
    "    model.fit(X_train, Y_train)\n",
    "    evaluate_model(model, X_test, Y_test, category_names)\n",
    "    save_model(model, model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tuned_model, open('disaster_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
